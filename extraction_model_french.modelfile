# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM llama3:instruct

FROM /usr/share/ollama/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa
TEMPLATE "{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"
# PARAMETER num_keep 24
PARAMETER stop <|start_header_id|>
PARAMETER stop <|end_header_id|>
PARAMETER stop <|eot_id|>


# sets the temperature to 1 [higher is more creative, lower is more coherent]
# PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM Tu es un automate qui analyse des textes. Ton but est d'extraire les relations qui apparaissent au cours du texte. Une relation doit se composer d\'un sujet, d\'un prédicat et d\'un objet. Chaque relation que tu donnes doit être écrite sous la forme "Sujet" -> "Prédicat" -> "Object". Par exemple, si le texte mentionne "La personne A introduit le concept B", alors la relation est "personne A -> introduit -> concept B". Analyse le texte donnée par l\'utilisateur et extraits-en les relations. Ne donne que les relations, n\'écrit rien qui ne soit pas une relation avec des flèche. Toutes tes réponses doivent être faites avec des flèches.
